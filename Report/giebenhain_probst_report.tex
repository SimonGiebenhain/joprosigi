% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Advanced Data Challenge Seminar\\
		Avito Demand Prediction}
%

% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Simon Giebenhain \and Jonas Probst}
%

% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{University of Konstanz\\
\email{simon.giebenhain@uni-konstanz.de}\\
\email{jonas.probst@uni-konstanz.de}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
This report gives a step-by-step description on how we tackled the Avito Demand Prediction Challenge on Kaggle and gives an overview of the used models, one LightGBM and one Neural Network. The goal of the challenge was to predict 'deal probability' (the probability that an ad actually sold something) for an advertisement on Avito based the ads parameters. 

\keywords{Kaggle \and Avito \and Demand Prediction \and Data Challenge}
\end{abstract}
%
%
%
\section{Dataset and Data Schema}
\subsection{Dataset}
Kaggle provided the following files for the challenge:
\begin{itemize}
	\item \textbf{train.csv:} Contains approximately 1.5 million training examples.
	\item \textbf{test.csv:} Contains approximately 500 000 test examples.
	\item \textbf{train\_active and test\_active:} Supplemental Data from ads that were displayed in the same time periods.
	\item \textbf{periods\_train and periods\_test:} Supplemental data showing the dates when the ads from train\_active and test\_active were activated and displayed.
	\item \textbf{train\_jpg.zip and test\_jpg.zip:} Images from ads in train.csv and test.csv.
	\item \textbf{sample\_submission.csv:} A sample submission in the correct format.\\
	\begin{center}
	\begin{tabular}{|c|c|}
		\hline 
		\textbf{item\_id} & \textbf{deal\_probability} \\ 
		\hline 
		1 & 0 \\ 
		\hline 
		4 & 0.1 \\ 
		\hline 
		126 & 0.6 \\ 
		\hline 
		... & ... \\ 
		\hline 
	\end{tabular} 
	\end{center} 
\end{itemize}

\subsection{Data Schema}  

Each data sample in train.csv and test.csv corresponds to one advertisement. The features are described in the following table.


\begin{table}
\caption{Data Schema}


\begin{tabular}{|c|c|c|}
	\hline 
	\textbf{Name} & \textbf{Data Type} & \textbf{Description} \\ 
	\hline 
	item\_id & String & Ad ID. \\ 
	\hline 
	user\_id & String & User ID. \\ 
	\hline 
	region & String & Ad region. \\ 
	\hline 
	city & String & Ad city. \\ 
	\hline 
	parent\_category\_name & String & Top level category. \\ 
	\hline 
	category\_name & String & More fine grained category. \\ 
	\hline 
	param\_1 & String & Optional parameter. \\ 
	\hline 
	param\_2 & String & Optional parameter. \\ 
	\hline 
	param\_3 & String & Optional parameter. \\ 
	\hline 
	title & String & Ad title. \\ 
	\hline 
	description & String & Ad description. \\ 
	\hline 
	price & Float & Ad price. \\ 
	\hline 
	item\_seq\_number & Integer & Ad seqential number for user. \\ 
	\hline 
	activation\_date & Date & Date ad was placed. \\ 
	\hline 
	user\_type & String & User type (Private, Company or Shop) \\ 
	\hline 
	image & String & ID of image. Same as filename. \\ 
	\hline 
	image\_top\_1 & Float & Mystery classification code for image. \\ 
	\hline 
	deal\_probability & Float & \begin{tabular}[c]{@{}c@{}}Target variable.\\ The likelihood that an ad actually sold something. \\No information on how it got calculated.\end{tabular} \\ 
	\hline 
\end{tabular} 
\end{table}

\section{Data Preprocessing}
\subsection{Data Preprocessing for LightGBM}
\subsubsection{Categorical Features:}
LightGBM only works on numerical data, so categorical data needs to be encoded. We used target encoding on "region", "city", "parent\_category\_name", "category\_name", "user\_type", "param\_1", "param\_2", "param\_3", "image\_top\_1".\\
Target encoding replaces the original value with the mean of the target variable value of all columns with the same original value.
\subsubsection{Numerical Features:}
Because you can sell anything from clothes to houses on Avito, "price" has a huge range, so we normalized it logarithmically.\\
The other numerical features did not need to be preprocessed.
\subsubsection{Activaton Date:} 
We extracted the weekday from "activation\_date" as a number from 0 to 6 and just kept that number, because month or day in month did not repeat.

\subsubsection{Missing Values}
 For most columns there are no missing values, because they are mandatory to fill in for uploading an ad or get filled in automatically. "param\_2" and "param\_3" have about 50\% missing values, because they are optional or don't exist for some categories. The Description was empty in about 8\%. We filled all these missing values with the empty string, as there is no good way to approximate these values and it may not even be desired because having no description is an interesting information itself.\\ \\
 Image and "image\_top\_1" were also missing in about 8\% of ads, which simply were uploaded without image. Here we actually trained a neural network on the description, title and optional parameters for ads with images to predict "image\_top\_1" because it was such an important variable.\\ \\
 Price is missing for about 5\%  of ads. There could be multiple reasons for that, for example the price could just be written in the description, could be up for debate our could depend on the amount one wants to buy. So we replaced missing values with -1, because setting it to zero would imply that the item was put up for free.
 
\section{Models}
\subsection{LightGBM Model}
LightGBM is a gradient boosting model developed by Microsoft which uses decision tree forests to make predictions. It performs well on huge amounts of data while still being relatively fast to train and is easily accessible. Because the given dataset is very large LightGBM seemed like a good choice. \\\\
We started our model with just the basic features that were given in the main data frame. From there we gradually added more features through feature engineering and external data.
\subsubsection{Baseline Model:} For the first model we just used the normalized price and the encoded categorical data, ignoring text and image features and the supplemental data. \\
Test Error: 0.231
\subsubsection{Basic Text Features:} We proceeded with adding basic text features, by extracting number of words, number of characters and number of unique words from description and title. \\
Test Error: 0.230
\subsubsection{TF-IDF:} The next step to get better understanding of the text data was to add Term Frequency - Inverse Document Frequency data, which gives us information about how strongly a document is defined by a specific word or term.\\
We restricted the TF-IDF to 50000 features for description and 50000 features for title. For both we allowed single words as well as 2-grams and 3-grams.\\
Test Error: 0.225\\
\subsubsection{Stemming:} Because in Russian the word endings depend on the grammatical case they are used in we tried to improve the TF-IDF by using stemming to bring all words in their basic form.
Test Error: 0.224
\subsubsection{Image Features:} From the images, we extracted size, blurriness, basic statistical features for color and brightness (mean, standard deviation, mode, min, max) as well as the top 3 predictions of Resnet, Xception, and Inception\_Resnet. Those features did not improve the model a lot, so we concluded that most of the image features were already encoded in "image\_top\_1".\\
Test Error: 0.2237 
\subsubsection{Aggregated Features:} From the supplemental data we were given we computed for each user and category the average online time for items, the average amount of times the same item was put up for sale and the number of items put up.\\
Test Error: 0.2230
\subsubsection{Final Model:} In our final model we used all previous features, because all of them improved the model at least a bit. We then trained with lower learning rate and increased the number of leaves for the trees from 256 to 1024.\\
Test Error: 0.222
\end{document}